{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZOUOQOrFs3zn"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1u9QVVsShC9X"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMitx5qSgJk1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import base64\n",
    "import IPython\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import gym_solventx \n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.categorical_dqn import categorical_dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import categorical_q_network\n",
    "from tf_agents.policies import random_tf_policy\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import common\n",
    "\n",
    "tf.compat.v1.enable_v2_behavior()\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LmC0NDhdLIKY"
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HC1kNrOsLSIZ"
   },
   "outputs": [],
   "source": [
    "env_name = \"gym_solventx-v0\" # @param {type:\"string\"}\n",
    "num_iterations = 15000 # @param {type:\"integer\"}\n",
    "\n",
    "initial_collect_steps = 1000  # @param {type:\"integer\"} \n",
    "collect_steps_per_iteration = 1  # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 100000  # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,100,50)\n",
    "\n",
    "batch_size = 64  # @param {type:\"integer\"}\n",
    "learning_rate = 1e-3  # @param {type:\"number\"}\n",
    "gamma = 0.99\n",
    "log_interval = 25  # @param {type:\"integer\"}\n",
    "\n",
    "num_atoms = 51  # @param {type:\"integer\"}\n",
    "min_q_value = -20  # @param {type:\"integer\"}\n",
    "max_q_value = 20  # @param {type:\"integer\"}\n",
    "n_step_update = 2  # @param {type:\"integer\"}\n",
    "\n",
    "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
    "eval_interval = 1000  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VMsJC3DEgI0x"
   },
   "source": [
    "## Environment\n",
    "\n",
    "Load the environment as before, with one for training and one for evaluation. Here we use CartPole-v1 (vs. CartPole-v0 in the DQN tutorial), which has a larger max reward of 500 rather than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xp-Y4mD6eDhF"
   },
   "outputs": [],
   "source": [
    "train_py_env = suite_gym.load(env_name)\n",
    "eval_py_env = suite_gym.load(env_name)\n",
    "\n",
    "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
    "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9lW_OZYFR8A"
   },
   "source": [
    "## Agent\n",
    "\n",
    "C51 is a Q-learning algorithm based on DQN. Like DQN, it can be used on any environment with a discrete action space.\n",
    "\n",
    "The main difference between C51 and DQN is that rather than simply predicting the Q-value for each state-action pair, C51 predicts a histogram model for the probability distribution of the Q-value:\n",
    "\n",
    "![Example C51 Distribution](images/c51_distribution.png)\n",
    "\n",
    "By learning the distribution rather than simply the expected value, the algorithm is able to stay more stable during training, leading to improved final performance. This is particularly true in situations with bimodal or even multimodal value distributions, where a single average does not provide an accurate picture.\n",
    "\n",
    "In order to train on probability distributions rather than on values, C51 must perform some complex distributional computations in order to calculate its loss function. But don't worry, all of this is taken care of for you in TF-Agents!\n",
    "\n",
    "To create a C51 Agent, we first need to create a `CategoricalQNetwork`. The API of the `CategoricalQNetwork` is the same as that of the `QNetwork`, except that there is an additional argument `num_atoms`. This represents the number of support points in our probability distribution estimates. (The above image includes 10 support points, each represented by a vertical blue bar.) As you can tell from the name, the default number of atoms is 51.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TgkdEPg_muzV"
   },
   "outputs": [],
   "source": [
    "categorical_q_net = categorical_q_network.CategoricalQNetwork(\n",
    "    train_env.observation_spec(),\n",
    "    train_env.action_spec(),\n",
    "    num_atoms=num_atoms,\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z62u55hSmviJ"
   },
   "source": [
    "We also need an `optimizer` to train the network we just created, and a `train_step_counter` variable to keep track of how many times the network was updated.\n",
    "\n",
    "Note that one other significant difference from vanilla `DqnAgent` is that we now need to specify `min_q_value` and `max_q_value` as arguments. These specify the most extreme values of the support (in other words, the most extreme of the 51 atoms on either side). Make sure to choose these appropriately for your particular environment. Here we use -20 and 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbY4yrjTEyc9"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.compat.v2.Variable(0)\n",
    "\n",
    "agent = categorical_dqn_agent.CategoricalDqnAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    categorical_q_network=categorical_q_net,\n",
    "    optimizer=optimizer,\n",
    "    min_q_value=min_q_value,\n",
    "    max_q_value=max_q_value,\n",
    "    n_step_update=n_step_update,\n",
    "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
    "    gamma=gamma,\n",
    "    train_step_counter=train_step_counter)\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7O7F_HqiQ1G"
   },
   "source": [
    "One last thing to note is that we also added an argument to use n-step updates with $n$ = 2. In single-step Q-learning ($n$ = 1), we only compute the error between the Q-values at the current time step and the next time step using the single-step return (based on the Bellman optimality equation). The single-step return is defined as:\n",
    "\n",
    "$G_t = R_{t + 1} + \\gamma V(s_{t + 1})$\n",
    "\n",
    "where we define $V(s) = \\max_a{Q(s, a)}$.\n",
    "\n",
    "N-step updates involve expanding the standard single-step return function $n$ times:\n",
    "\n",
    "$G_t^n = R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\dots + \\gamma^n V(s_{t + n})$\n",
    "\n",
    "N-step updates enable the agent to bootstrap from further in the future, and with the right value of $n$, this often leads to faster learning.\n",
    "\n",
    "Although C51 and n-step updates are often combined with prioritized replay to form the core of the [Rainbow agent](https://arxiv.org/pdf/1710.02298.pdf), we saw no measurable improvement from implementing prioritized replay. Moreover, we find that when combining our C51 agent with n-step updates alone, our agent performs as well as other Rainbow agents on the sample of Atari environments we've tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "94rCXQtbUbXv"
   },
   "source": [
    "## Metrics and Evaluation\n",
    "\n",
    "The most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode, and we usually average this over a few episodes. We can compute the average return metric as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bitzHo5_UbXy"
   },
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def compute_avg_return(environment, policy, num_episodes=10):\n",
    "\n",
    "  total_return = 0.0\n",
    "  for _ in range(num_episodes):\n",
    "\n",
    "    time_step = environment.reset()\n",
    "    episode_return = 0.0\n",
    "\n",
    "    while not time_step.is_last():\n",
    "      action_step = policy.action(time_step)\n",
    "      time_step = environment.step(action_step.action)\n",
    "      episode_return += time_step.reward\n",
    "    total_return += episode_return\n",
    "\n",
    "  avg_return = total_return / num_episodes\n",
    "  return avg_return.numpy()[0]\n",
    "\n",
    "\n",
    "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())\n",
    "\n",
    "#compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
    "\n",
    "# Please also see the metrics module for standard implementations of different\n",
    "# metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLva6g2jdWgr"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "As in the DQN tutorial, set up the replay buffer and the initial data collection with the random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wr1KSAEGG4h9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\splathottam\\.conda\\envs\\gym_simulator\\lib\\site-packages\\scipy\\optimize\\nonlin.py:475: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  and dx_norm/self.x_rtol <= x_norm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec=agent.collect_data_spec,\n",
    "    batch_size=train_env.batch_size,\n",
    "    max_length=replay_buffer_capacity)\n",
    "\n",
    "def collect_step(environment, policy):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  # Add trajectory to the replay buffer\n",
    "  replay_buffer.add_batch(traj)\n",
    "\n",
    "for i in range(initial_collect_steps):\n",
    "    print(i)\n",
    "    collect_step(train_env, random_policy)\n",
    "\n",
    "# This loop is so common in RL, that we provide standard implementations of\n",
    "# these. For more details see the drivers module.\n",
    "\n",
    "# Dataset generates trajectories with shape [BxTx...] where\n",
    "# T = n_step_update + 1.\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    num_parallel_calls=3, sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Trajectory(step_type=<tf.Tensor: id=29515, shape=(64, 3), dtype=int32, numpy=\n",
       " array([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])>, observation=<tf.Tensor: id=29516, shape=(64, 3, 11), dtype=float32, numpy=\n",
       " array([[[4.4999999e-01, 1.5489134e-04, 1.1363220e-03, ...,\n",
       "          2.0000000e+00, 8.0000000e+00, 5.0000000e+00],\n",
       "         [5.0000000e-01, 1.5489134e-04, 1.1363220e-03, ...,\n",
       "          2.0000000e+00, 8.0000000e+00, 5.0000000e+00],\n",
       "         [5.0000000e-01, 1.5489134e-04, 1.1363220e-03, ...,\n",
       "          2.0000000e+00, 8.0000000e+00, 5.0000000e+00]],\n",
       " \n",
       "        [[2.5000000e-01, 1.5489134e-04, 9.4114941e-05, ...,\n",
       "          4.0000000e+00, 4.0000000e+00, 2.0000000e+00],\n",
       "         [2.5000000e-01, 1.2073769e-04, 9.4114941e-05, ...,\n",
       "          4.0000000e+00, 4.0000000e+00, 2.0000000e+00],\n",
       "         [2.5000000e-01, 9.4114941e-05, 9.4114941e-05, ...,\n",
       "          4.0000000e+00, 4.0000000e+00, 2.0000000e+00]],\n",
       " \n",
       "        [[2.0000000e-01, 6.4981752e-03, 7.8457467e-02, ...,\n",
       "          4.0000000e+00, 4.0000000e+00, 2.0000000e+00],\n",
       "         [2.0000000e-01, 6.4981752e-03, 7.8457467e-02, ...,\n",
       "          4.0000000e+00, 3.0000000e+00, 2.0000000e+00],\n",
       "         [2.0000000e-01, 6.4981752e-03, 7.8457467e-02, ...,\n",
       "          4.0000000e+00, 3.0000000e+00, 2.0000000e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[4.0000001e-01, 1.2073769e-04, 1.4577589e-03, ...,\n",
       "          3.0000000e+00, 6.0000000e+00, 3.0000000e+00],\n",
       "         [4.0000001e-01, 1.2073769e-04, 1.4577589e-03, ...,\n",
       "          3.0000000e+00, 6.0000000e+00, 3.0000000e+00],\n",
       "         [4.0000001e-01, 1.2073769e-04, 1.4577589e-03, ...,\n",
       "          3.0000000e+00, 6.0000000e+00, 4.0000000e+00]],\n",
       " \n",
       "        [[4.4999999e-01, 5.0653229e-03, 6.1157539e-02, ...,\n",
       "          6.0000000e+00, 8.0000000e+00, 3.0000000e+00],\n",
       "         [4.4999999e-01, 5.0653229e-03, 6.1157539e-02, ...,\n",
       "          6.0000000e+00, 8.0000000e+00, 3.0000000e+00],\n",
       "         [4.4999999e-01, 5.0653229e-03, 6.1157539e-02, ...,\n",
       "          6.0000000e+00, 8.0000000e+00, 3.0000000e+00]],\n",
       " \n",
       "        [[2.0000000e-01, 6.4981752e-03, 6.1157539e-02, ...,\n",
       "          7.0000000e+00, 5.0000000e+00, 4.0000000e+00],\n",
       "         [2.5000000e-01, 6.4981752e-03, 6.1157539e-02, ...,\n",
       "          7.0000000e+00, 5.0000000e+00, 4.0000000e+00],\n",
       "         [2.5000000e-01, 5.0653229e-03, 6.1157539e-02, ...,\n",
       "          7.0000000e+00, 5.0000000e+00, 4.0000000e+00]]], dtype=float32)>, action=<tf.Tensor: id=29517, shape=(64, 3), dtype=int64, numpy=\n",
       " array([[ 0, 11,  9],\n",
       "        [ 3,  3,  4],\n",
       "        [19, 14,  3],\n",
       "        [ 9, 13,  2],\n",
       "        [13,  3, 12],\n",
       "        [15, 11, 22],\n",
       "        [21, 15, 15],\n",
       "        [11,  4,  1],\n",
       "        [ 4,  4,  6],\n",
       "        [11, 10, 12],\n",
       "        [14, 18, 16],\n",
       "        [15, 15,  1],\n",
       "        [ 4, 15, 21],\n",
       "        [17, 21,  2],\n",
       "        [19, 16, 20],\n",
       "        [13,  0, 11],\n",
       "        [13, 19,  0],\n",
       "        [12,  3, 22],\n",
       "        [ 7,  8,  1],\n",
       "        [11,  5,  4],\n",
       "        [14,  3, 14],\n",
       "        [13, 21, 22],\n",
       "        [ 8, 12,  4],\n",
       "        [15, 13,  8],\n",
       "        [14, 18,  6],\n",
       "        [ 8,  6,  1],\n",
       "        [ 7,  3,  3],\n",
       "        [17, 14,  3],\n",
       "        [11, 17,  1],\n",
       "        [16, 15, 21],\n",
       "        [18,  3, 17],\n",
       "        [19,  5, 18],\n",
       "        [ 3,  5, 14],\n",
       "        [20, 10, 13],\n",
       "        [ 9,  1, 13],\n",
       "        [13, 21,  6],\n",
       "        [ 0, 20,  7],\n",
       "        [ 0,  3, 15],\n",
       "        [10, 21, 13],\n",
       "        [21,  7,  6],\n",
       "        [21,  1, 17],\n",
       "        [15, 18, 15],\n",
       "        [14, 10,  9],\n",
       "        [11, 14,  8],\n",
       "        [ 2, 18, 19],\n",
       "        [ 3,  7,  1],\n",
       "        [ 6, 11, 11],\n",
       "        [15,  3,  0],\n",
       "        [22, 12, 22],\n",
       "        [ 9,  4, 16],\n",
       "        [16,  7,  5],\n",
       "        [ 5, 20, 13],\n",
       "        [10, 12, 16],\n",
       "        [ 2, 19, 19],\n",
       "        [ 1, 16, 19],\n",
       "        [14,  9, 13],\n",
       "        [ 2,  6,  5],\n",
       "        [22,  2,  3],\n",
       "        [19,  3, 14],\n",
       "        [ 7, 17,  1],\n",
       "        [17, 20,  1],\n",
       "        [ 7, 20, 13],\n",
       "        [ 8, 13, 16],\n",
       "        [ 0,  3, 15]], dtype=int64)>, policy_info=(), next_step_type=<tf.Tensor: id=29518, shape=(64, 3), dtype=int32, numpy=\n",
       " array([[1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1],\n",
       "        [1, 1, 1]])>, reward=<tf.Tensor: id=29519, shape=(64, 3), dtype=float32, numpy=\n",
       " array([[-16.196562, -16.196535, -16.1968  ],\n",
       "        [-16.190151, -16.190153, -16.190153],\n",
       "        [-15.031058, -15.032621, -15.024542],\n",
       "        [-16.19223 , -16.192232, -16.192232],\n",
       "        [-16.183987, -16.183987, -16.183987],\n",
       "        [-12.845577, -12.947237, -12.947237],\n",
       "        [-16.186003, -16.187431, -16.18888 ],\n",
       "        [-16.141273, -16.141731, -16.120234],\n",
       "        [-16.178696, -16.178694, -16.178654],\n",
       "        [-16.137516, -16.13751 , -16.140612],\n",
       "        [-13.748181, -13.74785 , -13.704993],\n",
       "        [-16.191015, -16.193386, -16.193386],\n",
       "        [-16.195248, -16.19737 , -16.19737 ],\n",
       "        [-16.000517, -16.000517, -16.000635],\n",
       "        [-16.185808, -16.186106, -16.186106],\n",
       "        [-16.184689, -16.184689, -16.18469 ],\n",
       "        [-16.193472, -16.193472, -16.19384 ],\n",
       "        [-15.673532, -15.672809, -15.672809],\n",
       "        [-16.184877, -16.18421 , -16.184145],\n",
       "        [-16.191772, -16.191772, -16.191772],\n",
       "        [-16.08853 , -16.0883  , -16.080109],\n",
       "        [-16.196415, -16.196415, -16.196415],\n",
       "        [-16.191774, -16.191774, -16.191774],\n",
       "        [-16.18888 , -16.18888 , -16.188396],\n",
       "        [-16.178055, -16.178055, -16.160322],\n",
       "        [-16.158028, -16.098335, -16.06212 ],\n",
       "        [-16.175646, -16.17559 , -16.175549],\n",
       "        [-16.097479, -16.08853 , -16.0883  ],\n",
       "        [-10.719213, -10.758129, -10.758129],\n",
       "        [-16.159822, -16.166676, -16.166676],\n",
       "        [-16.19397 , -16.19397 , -16.193836],\n",
       "        [-16.187805, -16.187805, -16.187805],\n",
       "        [-15.024542, -14.950141, -14.952618],\n",
       "        [-16.193474, -16.193474, -16.193472],\n",
       "        [-12.230712, -10.928546, -10.69407 ],\n",
       "        [-13.451707, -13.451895, -10.970796],\n",
       "        [-16.186804, -16.186804, -16.186804],\n",
       "        [ -9.668176,  -9.650232,  -9.578644],\n",
       "        [-16.191984, -16.191984, -16.191984],\n",
       "        [-16.196442, -16.196457, -16.196442],\n",
       "        [-16.196045, -16.196327, -16.196327],\n",
       "        [-16.178457, -16.178457, -16.180141],\n",
       "        [ -6.204607,  -6.192894,  -6.267726],\n",
       "        [-12.072809, -12.106004, -12.073901],\n",
       "        [-15.410854, -15.410833, -15.410857],\n",
       "        [-14.454132, -15.376009, -14.924046],\n",
       "        [-16.160322, -16.160212, -16.160093],\n",
       "        [-15.247107, -15.208066, -14.671598],\n",
       "        [-15.986149, -15.997502, -15.997502],\n",
       "        [-16.175846, -16.175837, -16.177273],\n",
       "        [-14.540134, -15.516089, -15.436548],\n",
       "        [-16.195908, -16.195908, -16.195906],\n",
       "        [-16.187712, -16.187712, -16.18793 ],\n",
       "        [-16.085596, -16.085596, -16.085596],\n",
       "        [-13.594918, -13.594918, -13.594682],\n",
       "        [-16.17682 , -16.177755, -16.17776 ],\n",
       "        [-16.134592, -16.048462, -16.047098],\n",
       "        [-16.161432, -16.16148 , -16.161432],\n",
       "        [-16.197947, -16.197947, -16.196087],\n",
       "        [-16.186804, -16.18685 , -16.18699 ],\n",
       "        [-16.189495, -16.18953 , -16.188555],\n",
       "        [-16.19562 , -16.19562 , -16.19562 ],\n",
       "        [-16.018448, -16.009544, -16.00822 ],\n",
       "        [ -9.668176,  -9.650232,  -9.578644]], dtype=float32)>, discount=<tf.Tensor: id=29520, shape=(64, 3), dtype=float32, numpy=\n",
       " array([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]], dtype=float32)>),\n",
       " BufferInfo(ids=<tf.Tensor: id=29521, shape=(64, 3), dtype=int64, numpy=\n",
       " array([[459, 460, 461],\n",
       "        [244, 245, 246],\n",
       "        [577, 578, 579],\n",
       "        [407, 408, 409],\n",
       "        [127, 128, 129],\n",
       "        [761, 762, 763],\n",
       "        [147, 148, 149],\n",
       "        [892, 893, 894],\n",
       "        [ 50,  51,  52],\n",
       "        [850, 851, 852],\n",
       "        [626, 627, 628],\n",
       "        [253, 254, 255],\n",
       "        [282, 283, 284],\n",
       "        [966, 967, 968],\n",
       "        [141, 142, 143],\n",
       "        [932, 933, 934],\n",
       "        [263, 264, 265],\n",
       "        [799, 800, 801],\n",
       "        [116, 117, 118],\n",
       "        [229, 230, 231],\n",
       "        [981, 982, 983],\n",
       "        [487, 488, 489],\n",
       "        [412, 413, 414],\n",
       "        [149, 150, 151],\n",
       "        [935, 936, 937],\n",
       "        [873, 874, 875],\n",
       "        [915, 916, 917],\n",
       "        [980, 981, 982],\n",
       "        [697, 698, 699],\n",
       "        [941, 942, 943],\n",
       "        [219, 220, 221],\n",
       "        [159, 160, 161],\n",
       "        [579, 580, 581],\n",
       "        [259, 260, 261],\n",
       "        [667, 668, 669],\n",
       "        [693, 694, 695],\n",
       "        [177, 178, 179],\n",
       "        [651, 652, 653],\n",
       "        [208, 209, 210],\n",
       "        [292, 293, 294],\n",
       "        [476, 477, 478],\n",
       "        [ 83,  84,  85],\n",
       "        [502, 503, 504],\n",
       "        [525, 526, 527],\n",
       "        [615, 616, 617],\n",
       "        [556, 557, 558],\n",
       "        [937, 938, 939],\n",
       "        [744, 745, 746],\n",
       "        [806, 807, 808],\n",
       "        [ 20,  21,  22],\n",
       "        [787, 788, 789],\n",
       "        [305, 306, 307],\n",
       "        [192, 193, 194],\n",
       "        [987, 988, 989],\n",
       "        [690, 691, 692],\n",
       "        [ 70,  71,  72],\n",
       "        [840, 841, 842],\n",
       "        [825, 826, 827],\n",
       "        [288, 289, 290],\n",
       "        [179, 180, 181],\n",
       "        [247, 248, 249],\n",
       "        [433, 434, 435],\n",
       "        [903, 904, 905],\n",
       "        [651, 652, 653]], dtype=int64)>, probabilities=<tf.Tensor: id=29522, shape=(64,), dtype=float32, numpy=\n",
       " array([0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002, 0.001002, 0.001002,\n",
       "        0.001002, 0.001002, 0.001002, 0.001002], dtype=float32)>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBc9lj9VWWtZ"
   },
   "source": [
    "## Training the agent\n",
    "\n",
    "The training loop involves both collecting data from the environment and optimizing the agent's networks. Along the way, we will occasionally evaluate the agent's policy to see how we are doing.\n",
    "\n",
    "The following will take ~7 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pTbJ3PeyF-u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 25: loss = 0.16028697788715363\n",
      "step = 50: loss = 0.17000138759613037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\splathottam\\.conda\\envs\\gym_simulator\\lib\\site-packages\\scipy\\optimize\\nonlin.py:475: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  and dx_norm/self.x_rtol <= x_norm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 75: loss = 0.12941479682922363\n",
      "step = 100: loss = 0.0668918713927269\n",
      "step = 125: loss = 0.00365733471699059\n",
      "step = 150: loss = 0.0724501684308052\n",
      "step = 175: loss = 0.011846370063722134\n",
      "step = 200: loss = 0.015369021333754063\n",
      "step = 225: loss = 0.004074460826814175\n",
      "step = 250: loss = 0.13777358829975128\n",
      "step = 275: loss = 0.0031425615306943655\n",
      "step = 300: loss = 0.009174241684377193\n",
      "step = 325: loss = 0.00722520612180233\n",
      "step = 350: loss = 0.07669011503458023\n",
      "step = 375: loss = 0.0679047629237175\n",
      "step = 400: loss = 0.005140916910022497\n",
      "step = 425: loss = 0.04810731112957001\n",
      "step = 450: loss = 0.1225263774394989\n",
      "step = 475: loss = 0.006990170571953058\n",
      "step = 500: loss = 0.12083490937948227\n",
      "step = 525: loss = 0.0073608895763754845\n",
      "step = 550: loss = 0.004039769060909748\n",
      "step = 575: loss = 0.062206756323575974\n",
      "step = 600: loss = 0.03520073741674423\n",
      "step = 625: loss = 0.007976322434842587\n",
      "step = 650: loss = 0.0022611175663769245\n",
      "step = 675: loss = 0.06574133783578873\n",
      "step = 700: loss = 0.2771194279193878\n",
      "step = 725: loss = 0.059062547981739044\n",
      "step = 750: loss = 0.0005793925374746323\n",
      "step = 775: loss = 0.012636600993573666\n",
      "step = 800: loss = 0.005454541649669409\n",
      "step = 825: loss = 0.1481807827949524\n",
      "step = 850: loss = 0.0020869458094239235\n",
      "step = 875: loss = 0.0015777975786477327\n",
      "step = 900: loss = 0.0042157783173024654\n",
      "step = 925: loss = 0.07715196162462234\n",
      "step = 950: loss = 0.017272375524044037\n",
      "step = 975: loss = 0.004542935639619827\n",
      "step = 1000: loss = 0.0031869790982455015\n",
      "step = 1000: Average Return = -7541.44\n",
      "step = 1025: loss = 0.0023360378108918667\n",
      "step = 1050: loss = 0.0469776950776577\n",
      "step = 1075: loss = 0.0005779887433163822\n",
      "step = 1100: loss = 0.001392524573020637\n",
      "step = 1125: loss = 0.021097537130117416\n",
      "step = 1150: loss = 0.004970072768628597\n",
      "step = 1175: loss = 0.0006152556161396205\n",
      "step = 1200: loss = 0.08838005363941193\n",
      "step = 1225: loss = 0.0014176140539348125\n",
      "step = 1250: loss = 0.0006351495976559818\n",
      "step = 1275: loss = 0.09985692799091339\n",
      "step = 1300: loss = 0.03221214562654495\n",
      "step = 1325: loss = 0.013692157343029976\n",
      "step = 1350: loss = 0.016811920329928398\n",
      "step = 1375: loss = 0.0565030574798584\n",
      "step = 1400: loss = 0.007920362986624241\n",
      "step = 1425: loss = 0.0006600756896659732\n",
      "step = 1450: loss = 0.004910605493932962\n",
      "step = 1475: loss = 0.0013034502044320107\n",
      "step = 1500: loss = 0.0010393981356173754\n",
      "step = 1525: loss = 0.1018347442150116\n",
      "step = 1550: loss = 0.001106576295569539\n",
      "step = 1575: loss = 0.00143028877209872\n",
      "step = 1600: loss = 0.012332635931670666\n",
      "step = 1625: loss = 0.021564403548836708\n",
      "step = 1650: loss = 0.05974791571497917\n",
      "step = 1675: loss = 0.06330905109643936\n",
      "step = 1700: loss = 0.13318586349487305\n",
      "step = 1725: loss = 0.0666017085313797\n",
      "step = 1750: loss = 0.00309513951651752\n",
      "step = 1775: loss = 0.00027289922581985593\n",
      "step = 1800: loss = 0.034272532910108566\n",
      "step = 1825: loss = 0.0004089566646143794\n",
      "step = 1850: loss = 0.09901461005210876\n",
      "step = 1875: loss = 0.08104989677667618\n",
      "step = 1900: loss = 0.009418005123734474\n",
      "step = 1925: loss = 0.09123249351978302\n",
      "step = 1950: loss = 0.0017047787550836802\n",
      "step = 1975: loss = 0.17065249383449554\n",
      "step = 2000: loss = 0.000985998078249395\n",
      "step = 2000: Average Return = -6664.47\n",
      "step = 2025: loss = 0.09904595464468002\n",
      "step = 2050: loss = 0.0020669137593358755\n",
      "step = 2075: loss = 0.06751570850610733\n",
      "step = 2100: loss = 0.060563426464796066\n",
      "step = 2125: loss = 0.007564648054540157\n",
      "step = 2150: loss = 0.06793263554573059\n",
      "step = 2175: loss = 0.003411006648093462\n",
      "step = 2200: loss = 0.06376025825738907\n",
      "step = 2225: loss = 0.03938080742955208\n",
      "step = 2250: loss = 0.11336100101470947\n",
      "step = 2275: loss = 0.0033589834347367287\n",
      "step = 2300: loss = 0.0008325177477672696\n",
      "step = 2325: loss = 0.0003684965777210891\n",
      "step = 2350: loss = 0.019392313435673714\n",
      "step = 2375: loss = 0.0010341918095946312\n",
      "step = 2400: loss = 0.0007048249244689941\n",
      "step = 2425: loss = 0.011014689691364765\n",
      "step = 2450: loss = 0.031047159805893898\n",
      "step = 2475: loss = 0.0010982335079461336\n",
      "step = 2500: loss = 0.005796052981168032\n",
      "step = 2525: loss = 0.0019515564199537039\n",
      "step = 2550: loss = 0.0029388265684247017\n",
      "step = 2575: loss = 0.016602685675024986\n",
      "step = 2600: loss = 0.0004903869703412056\n",
      "step = 2625: loss = 0.044736169278621674\n",
      "step = 2650: loss = 0.003901790827512741\n",
      "step = 2675: loss = 9.520867752144113e-05\n",
      "step = 2700: loss = 0.012770092114806175\n",
      "step = 2725: loss = 5.803331077913754e-05\n",
      "step = 2750: loss = 0.023990370333194733\n",
      "step = 2775: loss = 0.07134519517421722\n",
      "step = 2800: loss = 0.0007849904941394925\n",
      "step = 2825: loss = 0.0010341680608689785\n",
      "step = 2850: loss = 0.014450298622250557\n",
      "step = 2875: loss = 0.005209770984947681\n",
      "step = 2900: loss = 0.023023735731840134\n",
      "step = 2925: loss = 0.0003153356665279716\n",
      "step = 2950: loss = 0.0039792051538825035\n",
      "step = 2975: loss = 0.02132592163980007\n",
      "step = 3000: loss = 0.09056467562913895\n",
      "step = 3000: Average Return = -6847.00\n",
      "step = 3025: loss = 0.051728300750255585\n",
      "step = 3050: loss = 0.06580348312854767\n",
      "step = 3075: loss = 0.030686073005199432\n",
      "step = 3100: loss = 0.001097347354516387\n",
      "step = 3125: loss = 0.0010798863368108869\n",
      "step = 3150: loss = 0.018808860331773758\n",
      "step = 3175: loss = 7.41203548386693e-05\n",
      "step = 3200: loss = 0.003028146456927061\n",
      "step = 3225: loss = 0.00011585321772145107\n",
      "step = 3250: loss = 0.00433275755494833\n",
      "step = 3275: loss = 0.00014483972336165607\n",
      "step = 3300: loss = 0.11520072817802429\n",
      "step = 3325: loss = 0.003938568755984306\n",
      "step = 3350: loss = 0.0031898031011223793\n",
      "step = 3375: loss = 0.0013839385937899351\n",
      "step = 3400: loss = 0.0037943481002002954\n",
      "step = 3425: loss = 0.054082706570625305\n",
      "step = 3450: loss = 0.0004868226242251694\n",
      "step = 3475: loss = 0.0023459389340132475\n",
      "step = 3500: loss = 0.03363516181707382\n",
      "step = 3525: loss = 0.4241667687892914\n",
      "step = 3550: loss = 0.0016154700424522161\n",
      "step = 3575: loss = 0.0051953112706542015\n",
      "step = 3600: loss = 0.04732006788253784\n",
      "step = 3625: loss = 0.0022578199859708548\n",
      "step = 3650: loss = 0.06303588300943375\n",
      "step = 3675: loss = 0.08783503621816635\n",
      "step = 3700: loss = 0.07165470719337463\n",
      "step = 3725: loss = 0.07282528281211853\n",
      "step = 3750: loss = 0.009870665147900581\n",
      "step = 3775: loss = 0.0959153026342392\n",
      "step = 3800: loss = 0.09628080576658249\n",
      "step = 3825: loss = 0.042042192071676254\n",
      "step = 3850: loss = 0.05966750159859657\n",
      "step = 3875: loss = 0.06692900508642197\n",
      "step = 3900: loss = 0.033818263560533524\n",
      "step = 3925: loss = 0.03357124328613281\n",
      "step = 3950: loss = 0.04767847806215286\n",
      "step = 3975: loss = 0.06350256502628326\n",
      "step = 4000: loss = 0.004981200210750103\n",
      "step = 4000: Average Return = -7337.95\n",
      "step = 4025: loss = 0.18807733058929443\n",
      "step = 4050: loss = 0.00952241476625204\n",
      "step = 4075: loss = 0.014055486768484116\n",
      "step = 4100: loss = 0.06410853564739227\n",
      "step = 4125: loss = 0.015960803255438805\n",
      "step = 4150: loss = 0.05956701189279556\n",
      "step = 4175: loss = 0.0008765063248574734\n",
      "step = 4200: loss = 0.15962134301662445\n",
      "step = 4225: loss = 0.0703839585185051\n",
      "step = 4250: loss = 0.07962106168270111\n",
      "step = 4275: loss = 0.0034019018057733774\n",
      "step = 4300: loss = 0.07309562712907791\n",
      "step = 4325: loss = 0.08218993246555328\n",
      "step = 4350: loss = 0.025457093492150307\n",
      "step = 4375: loss = 0.025744540616869926\n",
      "step = 4400: loss = 0.003775316523388028\n",
      "step = 4425: loss = 0.015041309408843517\n",
      "step = 4450: loss = 0.010889538563787937\n",
      "step = 4475: loss = 0.0009987418306991458\n",
      "step = 4500: loss = 0.0011464804410934448\n",
      "step = 4525: loss = 0.06245607137680054\n",
      "step = 4550: loss = 0.07614652067422867\n",
      "step = 4575: loss = 0.06436780095100403\n",
      "step = 4600: loss = 0.001884559285826981\n",
      "step = 4625: loss = 0.11998996138572693\n",
      "step = 4650: loss = 0.0002877230872400105\n",
      "step = 4675: loss = 0.005881412886083126\n",
      "step = 4700: loss = 0.0005101453280076385\n",
      "step = 4725: loss = 0.005251983180642128\n",
      "step = 4750: loss = 0.01960930787026882\n",
      "step = 4775: loss = 0.009919553063809872\n",
      "step = 4800: loss = 0.0025735250674188137\n",
      "step = 4825: loss = 0.00292457127943635\n",
      "step = 4850: loss = 0.013223154470324516\n",
      "step = 4875: loss = 0.0071504185907542706\n",
      "step = 4900: loss = 0.0044907876290380955\n",
      "step = 4925: loss = 0.15551109611988068\n",
      "step = 4950: loss = 0.03670056164264679\n",
      "step = 4975: loss = 0.0009043271420523524\n",
      "step = 5000: loss = 0.01307656615972519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 5000: Average Return = -6671.47\n",
      "step = 5025: loss = 0.06812111288309097\n",
      "step = 5050: loss = 0.11946965754032135\n",
      "step = 5075: loss = 0.018798358738422394\n",
      "step = 5100: loss = 0.011560304090380669\n",
      "step = 5125: loss = 0.010643796063959599\n",
      "step = 5150: loss = 0.006489992141723633\n",
      "step = 5175: loss = 0.040706850588321686\n",
      "step = 5200: loss = 0.004066865425556898\n",
      "step = 5225: loss = 0.009515754878520966\n",
      "step = 5250: loss = 0.014009124599397182\n",
      "step = 5275: loss = 0.005462448112666607\n",
      "step = 5300: loss = 0.003895748872309923\n",
      "step = 5325: loss = 0.011694632470607758\n",
      "step = 5350: loss = 0.008766425773501396\n",
      "step = 5375: loss = 0.003997023217380047\n",
      "step = 5400: loss = 0.00822072010487318\n",
      "step = 5425: loss = 0.007190256379544735\n",
      "step = 5450: loss = 0.004354902543127537\n",
      "step = 5475: loss = 0.21717087924480438\n",
      "step = 5500: loss = 0.002996796276420355\n",
      "step = 5525: loss = 0.005599659867584705\n",
      "step = 5550: loss = 0.020398642867803574\n",
      "step = 5575: loss = 0.14360332489013672\n",
      "step = 5600: loss = 0.01600612886250019\n",
      "step = 5625: loss = 0.12751266360282898\n",
      "step = 5650: loss = 0.01808231510221958\n",
      "step = 5675: loss = 0.004454730078577995\n",
      "step = 5700: loss = 0.08820132166147232\n",
      "step = 5725: loss = 0.006014749873429537\n",
      "step = 5750: loss = 0.018661143258213997\n",
      "step = 5775: loss = 0.001593162538483739\n",
      "step = 5800: loss = 0.012118428014218807\n",
      "step = 5825: loss = 0.003372283186763525\n",
      "step = 5850: loss = 0.0048658172599971294\n",
      "step = 5875: loss = 0.08344009518623352\n",
      "step = 5900: loss = 0.006525062955915928\n",
      "step = 5925: loss = 0.055542949587106705\n",
      "step = 5950: loss = 0.030419472604990005\n",
      "step = 5975: loss = 0.07688005268573761\n",
      "step = 6000: loss = 0.0010151660535484552\n",
      "step = 6000: Average Return = -7592.33\n",
      "step = 6025: loss = 0.001834993134252727\n",
      "step = 6050: loss = 0.0007405856158584356\n",
      "step = 6075: loss = 0.1527569591999054\n",
      "step = 6100: loss = 0.0024378737434744835\n",
      "step = 6125: loss = 0.02119585871696472\n",
      "step = 6150: loss = 0.040714483708143234\n",
      "step = 6175: loss = 0.033816296607255936\n",
      "step = 6200: loss = 0.035879749804735184\n",
      "step = 6225: loss = 0.044796500355005264\n",
      "step = 6250: loss = 0.06760749965906143\n",
      "step = 6275: loss = 0.0058041587471961975\n",
      "step = 6300: loss = 0.05091645568609238\n",
      "step = 6325: loss = 0.13523587584495544\n",
      "step = 6350: loss = 0.0017570438794791698\n",
      "step = 6375: loss = 0.06098783016204834\n",
      "step = 6400: loss = 0.05836586654186249\n",
      "step = 6425: loss = 0.005040119402110577\n",
      "step = 6450: loss = 0.0012480393052101135\n",
      "step = 6475: loss = 0.0008096700767055154\n",
      "step = 6500: loss = 0.0038865674287080765\n",
      "step = 6525: loss = 0.011137675493955612\n",
      "step = 6550: loss = 0.008288249373435974\n",
      "step = 6575: loss = 0.1416013538837433\n",
      "step = 6600: loss = 0.017679212614893913\n",
      "step = 6625: loss = 0.04551512748003006\n",
      "step = 6650: loss = 0.011532116681337357\n",
      "step = 6675: loss = 0.017884159460663795\n",
      "step = 6700: loss = 0.016173643991351128\n",
      "step = 6725: loss = 0.06071591377258301\n",
      "step = 6750: loss = 0.006291544530540705\n",
      "step = 6775: loss = 0.0010083504021167755\n",
      "step = 6800: loss = 0.003964316099882126\n",
      "step = 6825: loss = 0.006609986070543528\n",
      "step = 6850: loss = 0.06472614407539368\n",
      "step = 6875: loss = 0.0035511215683072805\n",
      "step = 6900: loss = 0.07376029342412949\n",
      "step = 6925: loss = 0.029543519020080566\n",
      "step = 6950: loss = 0.05367670953273773\n",
      "step = 6975: loss = 0.001572932000271976\n",
      "step = 7000: loss = 0.034309517592191696\n",
      "step = 7000: Average Return = -7630.74\n",
      "step = 7025: loss = 0.00782607402652502\n",
      "step = 7050: loss = 0.004384905565530062\n",
      "step = 7075: loss = 0.004992484115064144\n",
      "step = 7100: loss = 0.0017778432229533792\n",
      "step = 7125: loss = 0.006918394006788731\n",
      "step = 7150: loss = 0.03747190535068512\n",
      "step = 7175: loss = 0.003302417229861021\n",
      "step = 7200: loss = 0.0020741417538374662\n",
      "step = 7225: loss = 0.07537497580051422\n",
      "step = 7250: loss = 0.0030398545786738396\n",
      "step = 7275: loss = 0.006868231110274792\n",
      "step = 7300: loss = 0.007164603099226952\n",
      "step = 7325: loss = 0.0015639020130038261\n",
      "step = 7350: loss = 0.002974587958306074\n",
      "step = 7375: loss = 0.04219501465559006\n",
      "step = 7400: loss = 0.11540485173463821\n",
      "step = 7425: loss = 0.05499830096960068\n",
      "step = 7450: loss = 0.0019383457256481051\n",
      "step = 7475: loss = 0.09959076344966888\n",
      "step = 7500: loss = 0.0003488919173832983\n",
      "step = 7525: loss = 0.0019133626483380795\n",
      "[True, True, True, True, True, True, False, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "step = 7550: loss = 0.04226556047797203\n",
      "step = 7575: loss = 0.002330809598788619\n",
      "step = 7600: loss = 0.04196355119347572\n",
      "step = 7625: loss = 0.022471223026514053\n",
      "step = 7650: loss = 0.07671068608760834\n",
      "step = 7675: loss = 0.005320525728166103\n",
      "step = 7700: loss = 0.0020563954021781683\n",
      "step = 7725: loss = 0.06228359416127205\n",
      "step = 7750: loss = 0.02163168229162693\n",
      "step = 7775: loss = 0.0006907880888320506\n",
      "step = 7800: loss = 0.007359152194112539\n",
      "step = 7825: loss = 0.018203724175691605\n",
      "step = 7850: loss = 0.001087491400539875\n",
      "step = 7875: loss = 0.06130954250693321\n",
      "step = 7900: loss = 0.014329781755805016\n",
      "step = 7925: loss = 0.016829051077365875\n",
      "step = 7950: loss = 0.026920542120933533\n",
      "step = 7975: loss = 0.001986305695027113\n",
      "step = 8000: loss = 0.010551640763878822\n",
      "step = 8000: Average Return = -7423.19\n",
      "step = 8025: loss = 0.0008876362699083984\n",
      "step = 8050: loss = 0.10287974774837494\n",
      "step = 8075: loss = 0.1433100551366806\n",
      "step = 8100: loss = 0.0027274421881884336\n",
      "step = 8125: loss = 0.008299421519041061\n",
      "step = 8150: loss = 0.11169236898422241\n",
      "step = 8175: loss = 0.0015753586776554585\n",
      "step = 8200: loss = 0.004750595893710852\n",
      "step = 8225: loss = 0.054257430136203766\n",
      "step = 8250: loss = 0.14305397868156433\n",
      "step = 8275: loss = 0.04990578815340996\n",
      "step = 8300: loss = 0.004533672239631414\n",
      "step = 8325: loss = 0.007626699283719063\n",
      "step = 8350: loss = 0.05569266527891159\n",
      "step = 8375: loss = 0.004445089492946863\n",
      "step = 8400: loss = 0.005276892334222794\n",
      "step = 8425: loss = 0.006047945003956556\n",
      "step = 8450: loss = 0.011671001091599464\n",
      "step = 8475: loss = 0.02750817872583866\n",
      "step = 8500: loss = 0.04413670301437378\n",
      "step = 8525: loss = 0.0035440782085061073\n",
      "step = 8550: loss = 0.030620655044913292\n",
      "step = 8575: loss = 0.013251451775431633\n",
      "step = 8600: loss = 0.05162542313337326\n",
      "step = 8625: loss = 0.11744289845228195\n",
      "step = 8650: loss = 0.14597314596176147\n",
      "step = 8675: loss = 0.012430651113390923\n",
      "step = 8700: loss = 0.07118598371744156\n",
      "step = 8725: loss = 0.0069096884690225124\n",
      "step = 8750: loss = 0.003670060308650136\n",
      "step = 8775: loss = 0.018669653683900833\n",
      "step = 8800: loss = 0.006676526740193367\n",
      "step = 8825: loss = 0.09752798825502396\n",
      "step = 8850: loss = 0.007759956177324057\n",
      "step = 8875: loss = 0.06159404292702675\n",
      "step = 8900: loss = 0.05163508653640747\n",
      "step = 8925: loss = 0.04845057800412178\n",
      "step = 8950: loss = 0.09941297769546509\n",
      "step = 8975: loss = 0.055253203958272934\n",
      "step = 9000: loss = 0.01997937262058258\n",
      "step = 9000: Average Return = -7585.54\n",
      "step = 9025: loss = 0.045387402176856995\n",
      "step = 9050: loss = 0.009311535395681858\n",
      "step = 9075: loss = 0.008413349278271198\n",
      "step = 9100: loss = 0.04664032906293869\n",
      "step = 9125: loss = 0.016535036265850067\n",
      "step = 9150: loss = 0.005842059850692749\n",
      "step = 9175: loss = 0.01182122528553009\n",
      "step = 9200: loss = 0.011268898844718933\n",
      "step = 9225: loss = 0.10399965941905975\n",
      "step = 9250: loss = 0.019240643829107285\n",
      "step = 9275: loss = 0.0014852310996502638\n",
      "step = 9300: loss = 0.003997402731329203\n",
      "step = 9325: loss = 0.020725887268781662\n",
      "step = 9350: loss = 0.013390921987593174\n",
      "step = 9375: loss = 0.049823563545942307\n",
      "step = 9400: loss = 0.08770385384559631\n",
      "step = 9425: loss = 0.04309726506471634\n",
      "step = 9450: loss = 0.10447216778993607\n",
      "step = 9475: loss = 0.14799414575099945\n",
      "step = 9500: loss = 0.04629102721810341\n",
      "step = 9525: loss = 0.0031474826391786337\n",
      "step = 9550: loss = 0.09203214198350906\n",
      "step = 9575: loss = 0.02017132379114628\n",
      "step = 9600: loss = 0.0728844627737999\n",
      "step = 9625: loss = 0.01077430509030819\n",
      "step = 9650: loss = 0.002305334433913231\n",
      "step = 9675: loss = 0.004701642785221338\n",
      "step = 9700: loss = 0.00885615311563015\n",
      "step = 9725: loss = 0.021191872656345367\n",
      "step = 9750: loss = 0.0013685140293091536\n",
      "step = 9775: loss = 0.045001737773418427\n",
      "step = 9800: loss = 0.055231958627700806\n",
      "step = 9825: loss = 0.07579226791858673\n",
      "step = 9850: loss = 0.008502897806465626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Failed!\n",
      "step = 9875: loss = 0.11926648020744324\n",
      "step = 9900: loss = 0.01573614776134491\n",
      "step = 9925: loss = 0.04951256141066551\n",
      "step = 9950: loss = 0.0020991340279579163\n",
      "step = 9975: loss = 0.0012989749666303396\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "step = 10000: loss = 0.005769754759967327\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, False, True, True, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, False, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "step = 10000: Average Return = -7547.24\n",
      "step = 10025: loss = 0.0061641111969947815\n",
      "step = 10050: loss = 0.04660315439105034\n",
      "step = 10075: loss = 0.040097031742334366\n",
      "step = 10100: loss = 0.0603143610060215\n",
      "step = 10125: loss = 0.03655853122472763\n",
      "step = 10150: loss = 0.027340903878211975\n",
      "step = 10175: loss = 0.005166562274098396\n",
      "step = 10200: loss = 0.007989838719367981\n",
      "step = 10225: loss = 0.00923723541200161\n",
      "step = 10250: loss = 0.010450806468725204\n",
      "step = 10275: loss = 0.09909884631633759\n",
      "step = 10300: loss = 0.15242938697338104\n",
      "step = 10325: loss = 0.04155610501766205\n",
      "step = 10350: loss = 0.03588500991463661\n",
      "step = 10375: loss = 0.06033528968691826\n",
      "step = 10400: loss = 0.03235824406147003\n",
      "step = 10425: loss = 0.004836771171540022\n",
      "step = 10450: loss = 0.0067689851857721806\n",
      "step = 10475: loss = 0.007633136585354805\n",
      "step = 10500: loss = 0.05117294192314148\n",
      "step = 10525: loss = 0.04546620696783066\n",
      "step = 10550: loss = 0.025344163179397583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 10575: loss = 0.12599144876003265\n",
      "step = 10600: loss = 0.2003866583108902\n",
      "step = 10625: loss = 0.10821938514709473\n",
      "step = 10650: loss = 0.07372990250587463\n",
      "step = 10675: loss = 0.13392683863639832\n",
      "step = 10700: loss = 0.07629528641700745\n",
      "step = 10725: loss = 0.08680525422096252\n",
      "step = 10750: loss = 0.04924719035625458\n",
      "step = 10775: loss = 0.04744254797697067\n",
      "step = 10800: loss = 0.06589168310165405\n",
      "step = 10825: loss = 0.011920892633497715\n",
      "step = 10850: loss = 0.026222866028547287\n",
      "step = 10875: loss = 0.08235630393028259\n",
      "[True, True, True, True, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "[True, True, True, True, False, True, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "step = 10900: loss = 0.018795037642121315\n",
      "step = 10925: loss = 0.02616961859166622\n",
      "step = 10950: loss = 0.06235100328922272\n",
      "step = 10975: loss = 0.18317431211471558\n",
      "step = 11000: loss = 0.020461346954107285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\splathottam\\.conda\\envs\\gym_simulator\\lib\\site-packages\\scipy\\optimize\\linesearch.py:711: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  alpha2 = (-b + np.sqrt(abs(b**2 - 3 * a * derphi0))) / (3.0*a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step = 11000: Average Return = -5758.80\n",
      "step = 11025: loss = 0.05469789355993271\n",
      "step = 11050: loss = 0.033082686364650726\n",
      "step = 11075: loss = 0.0375412181019783\n",
      "step = 11100: loss = 0.02889394387602806\n",
      "step = 11125: loss = 0.1186695545911789\n",
      "step = 11150: loss = 0.11857697367668152\n",
      "step = 11175: loss = 0.05395786464214325\n",
      "step = 11200: loss = 0.023146428167819977\n",
      "step = 11225: loss = 0.028384637087583542\n",
      "step = 11250: loss = 0.13244414329528809\n",
      "step = 11275: loss = 0.12826424837112427\n",
      "step = 11300: loss = 0.005163517780601978\n",
      "step = 11325: loss = 0.007408432196825743\n",
      "step = 11350: loss = 0.09879369288682938\n",
      "step = 11375: loss = 0.05789566785097122\n",
      "step = 11400: loss = 0.09281739592552185\n",
      "step = 11425: loss = 0.018958017230033875\n",
      "step = 11450: loss = 0.12496282905340195\n",
      "step = 11475: loss = 0.08919675648212433\n",
      "step = 11500: loss = 0.015369392931461334\n",
      "step = 11525: loss = 0.04276541620492935\n",
      "step = 11550: loss = 0.12305134534835815\n",
      "step = 11575: loss = 0.15409275889396667\n",
      "step = 11600: loss = 0.1741449236869812\n",
      "step = 11625: loss = 0.030467335134744644\n",
      "step = 11650: loss = 0.04399137198925018\n",
      "step = 11675: loss = 0.07003860175609589\n",
      "step = 11700: loss = 0.026967119425535202\n",
      "step = 11725: loss = 0.041569799184799194\n",
      "step = 11750: loss = 0.025116629898548126\n",
      "step = 11775: loss = 0.06008385866880417\n",
      "step = 11800: loss = 0.0028547996189445257\n",
      "step = 11825: loss = 0.0775366723537445\n",
      "step = 11850: loss = 0.03237593173980713\n",
      "step = 11875: loss = 0.0008395870681852102\n",
      "step = 11900: loss = 0.07104028761386871\n",
      "step = 11925: loss = 0.06244950741529465\n",
      "step = 11950: loss = 0.07385621219873428\n",
      "step = 11975: loss = 0.11557833105325699\n",
      "step = 12000: loss = 0.02577737346291542\n",
      "step = 12000: Average Return = -5402.82\n",
      "step = 12025: loss = 0.015585387125611305\n",
      "step = 12050: loss = 0.012562265619635582\n",
      "step = 12075: loss = 0.004305528476834297\n",
      "step = 12100: loss = 0.014078421518206596\n",
      "step = 12125: loss = 0.0058995638974010944\n",
      "step = 12150: loss = 0.004333294928073883\n",
      "step = 12175: loss = 0.005103989504277706\n",
      "step = 12200: loss = 0.03591816872358322\n",
      "step = 12225: loss = 0.07271646708250046\n",
      "step = 12250: loss = 0.17096339166164398\n",
      "step = 12275: loss = 0.0051550427451729774\n",
      "step = 12300: loss = 0.0025920611806213856\n",
      "step = 12325: loss = 0.004594908095896244\n",
      "step = 12350: loss = 0.023728180676698685\n",
      "step = 12375: loss = 0.026550332084298134\n",
      "step = 12400: loss = 0.02720123529434204\n",
      "step = 12425: loss = 0.025673966854810715\n",
      "step = 12450: loss = 0.012165484949946404\n",
      "step = 12475: loss = 0.00041428086115047336\n",
      "step = 12500: loss = 0.03595627471804619\n",
      "step = 12525: loss = 0.0034589883871376514\n",
      "step = 12550: loss = 0.012145385146141052\n",
      "step = 12575: loss = 0.003939421847462654\n",
      "step = 12600: loss = 0.115557461977005\n",
      "step = 12625: loss = 0.05721317604184151\n",
      "step = 12650: loss = 0.14299219846725464\n",
      "step = 12675: loss = 0.06493011862039566\n",
      "step = 12700: loss = 0.032383620738983154\n",
      "step = 12725: loss = 0.009905612096190453\n",
      "step = 12750: loss = 0.02446969598531723\n",
      "step = 12775: loss = 0.0024478579871356487\n",
      "step = 12800: loss = 0.06060666963458061\n",
      "step = 12825: loss = 0.0019116417970508337\n",
      "step = 12850: loss = 0.011274076998233795\n",
      "step = 12875: loss = 0.016990449279546738\n",
      "step = 12900: loss = 0.08239215612411499\n",
      "step = 12925: loss = 0.004723822232335806\n",
      "step = 12950: loss = 0.01648658514022827\n",
      "step = 12975: loss = 0.06867102533578873\n",
      "[True, True, True, False, False, True, True, True, True, True, True, True]\n",
      "Equilibrium Failed! Invalid State Reached!\n",
      "step = 13000: loss = 0.11070594936609268\n",
      "step = 13000: Average Return = -6273.22\n",
      "step = 13025: loss = 0.07831014692783356\n",
      "step = 13050: loss = 0.018680330365896225\n",
      "step = 13075: loss = 0.0017925766296684742\n",
      "step = 13100: loss = 0.07952806353569031\n",
      "step = 13125: loss = 0.3795161247253418\n",
      "step = 13150: loss = 0.02137976512312889\n",
      "step = 13175: loss = 0.0023661928717046976\n",
      "step = 13200: loss = 0.008669855073094368\n",
      "step = 13225: loss = 0.01518295519053936\n",
      "step = 13250: loss = 0.0979471355676651\n",
      "step = 13275: loss = 0.021213307976722717\n",
      "step = 13300: loss = 0.08031400293111801\n",
      "step = 13325: loss = 0.0017690262757241726\n",
      "step = 13350: loss = 0.01624966785311699\n",
      "step = 13375: loss = 0.026302864775061607\n",
      "step = 13400: loss = 0.005931362509727478\n",
      "step = 13425: loss = 0.06894183158874512\n",
      "step = 13450: loss = 0.020346278324723244\n",
      "step = 13475: loss = 0.02361755259335041\n",
      "step = 13500: loss = 0.1084563136100769\n",
      "step = 13525: loss = 0.07356841117143631\n",
      "step = 13550: loss = 0.1303776204586029\n",
      "step = 13575: loss = 0.012848676182329655\n",
      "step = 13600: loss = 0.0012401365675032139\n",
      "step = 13625: loss = 0.15143631398677826\n",
      "step = 13650: loss = 0.10373952239751816\n",
      "step = 13675: loss = 0.014389324933290482\n",
      "step = 13700: loss = 0.01007713284343481\n",
      "step = 13725: loss = 0.10813193768262863\n",
      "step = 13750: loss = 0.018894977867603302\n",
      "step = 13775: loss = 0.049911949783563614\n",
      "step = 13800: loss = 0.05193302780389786\n",
      "step = 13825: loss = 0.002152144443243742\n",
      "step = 13850: loss = 0.09775266796350479\n",
      "step = 13875: loss = 0.05106455087661743\n",
      "step = 13900: loss = 0.01507620234042406\n",
      "step = 13925: loss = 0.006627601571381092\n",
      "step = 13950: loss = 0.02315181866288185\n",
      "step = 13975: loss = 0.012267984449863434\n",
      "step = 14000: loss = 0.016247648745775223\n",
      "step = 14000: Average Return = -6263.22\n",
      "step = 14025: loss = 0.0006483151228167117\n",
      "step = 14050: loss = 0.032016582787036896\n",
      "step = 14075: loss = 0.14799034595489502\n",
      "step = 14100: loss = 0.07664820551872253\n",
      "step = 14125: loss = 0.020699260756373405\n",
      "step = 14150: loss = 0.07240724563598633\n",
      "step = 14175: loss = 0.058637335896492004\n",
      "step = 14200: loss = 0.06901492923498154\n",
      "step = 14225: loss = 0.06200012192130089\n",
      "step = 14250: loss = 0.0146805914118886\n",
      "step = 14275: loss = 0.05565953999757767\n",
      "step = 14300: loss = 0.0034515471197664738\n",
      "step = 14325: loss = 0.06017281487584114\n",
      "step = 14350: loss = 0.003645583288744092\n",
      "step = 14375: loss = 0.045690834522247314\n",
      "step = 14400: loss = 0.0020221162121742964\n",
      "step = 14425: loss = 0.003913490567356348\n",
      "step = 14450: loss = 0.06394245475530624\n",
      "step = 14475: loss = 0.04482914134860039\n",
      "step = 14500: loss = 0.003860385622829199\n",
      "step = 14525: loss = 0.17078985273838043\n",
      "step = 14550: loss = 0.0009489862713962793\n",
      "step = 14575: loss = 0.07923472672700882\n",
      "step = 14600: loss = 0.0038499946240335703\n",
      "step = 14625: loss = 0.005803661420941353\n",
      "step = 14650: loss = 0.06695973128080368\n",
      "step = 14675: loss = 0.008107233792543411\n",
      "step = 14700: loss = 0.017189867794513702\n",
      "step = 14725: loss = 0.001266684615984559\n",
      "step = 14750: loss = 0.1918662190437317\n",
      "step = 14775: loss = 0.08754300326108932\n",
      "step = 14800: loss = 0.1751190721988678\n",
      "step = 14825: loss = 0.11187249422073364\n",
      "step = 14850: loss = 0.06181757524609566\n",
      "step = 14875: loss = 0.08509894460439682\n",
      "step = 14900: loss = 0.0023784940131008625\n",
      "step = 14925: loss = 0.10396017134189606\n",
      "step = 14950: loss = 0.02596326172351837\n",
      "step = 14975: loss = 0.12281401455402374\n",
      "step = 15000: loss = 0.005458139814436436\n",
      "step = 15000: Average Return = -7720.25\n"
     ]
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "#%%time\n",
    "\n",
    "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
    "agent.train = common.function(agent.train)\n",
    "\n",
    "# Reset the train step\n",
    "agent.train_step_counter.assign(0)\n",
    "\n",
    "# Evaluate the agent's policy once before training.\n",
    "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "returns = [avg_return]\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "\n",
    "  # Collect a few steps using collect_policy and save to the replay buffer.\n",
    "  for _ in range(collect_steps_per_iteration):\n",
    "    collect_step(train_env, agent.collect_policy)\n",
    "\n",
    "  # Sample a batch of data from the buffer and update the agent's network.\n",
    "  experience, unused_info = next(iterator)\n",
    "  train_loss = agent.train(experience)\n",
    "\n",
    "  step = agent.train_step_counter.numpy()\n",
    "\n",
    "  if step % log_interval == 0:\n",
    "    print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "\n",
    "  if step % eval_interval == 0:\n",
    "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
    "    print('step = {0}: Average Return = {1:.2f}'.format(step, avg_return))\n",
    "    returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "68jNcA_TiJDq"
   },
   "source": [
    "## Visualization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aO-LWCdbbOIC"
   },
   "source": [
    "### Plots\n",
    "\n",
    "We can plot return vs global steps to see the performance of our agent. In `Cartpole-v1`, the environment gives a reward of +1 for every time step the pole stays up, and since the maximum number of steps is 500, the maximum possible return is also 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxtL1mbOYCVO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-7836.11650390625, 550)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#@test {\"skip\": true}\n",
    "\n",
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9pGfGxSH32gn"
   },
   "source": [
    "It is helpful to visualize the performance of an agent by rendering the environment at each step. Before we do that, let us first create a function to embed videos in this colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9c_PH-pX4Pr5"
   },
   "source": [
    "The following code visualizes the agent's policy for a few episodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "owOVWB158NlF"
   },
   "outputs": [],
   "source": [
    "num_episodes = 3\n",
    "video_filename = 'imageio.mp4'\n",
    "with imageio.get_writer(video_filename, fps=60) as video:\n",
    "  for _ in range(num_episodes):\n",
    "    time_step = eval_env.reset()\n",
    "    video.append_data(eval_py_env.render())\n",
    "    while not time_step.is_last():\n",
    "      action_step = agent.policy.action(time_step)\n",
    "      time_step = eval_env.step(action_step.action)\n",
    "      video.append_data(eval_py_env.render())\n",
    "\n",
    "embed_mp4(video_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "exziB27hY8ia"
   },
   "source": [
    "C51 tends to do slightly better than DQN on CartPole-v1, but the difference between the two agents becomes more and more significant in increasingly complex environments. For example, on the full Atari 2600 benchmark, C51 demonstrates a mean score improvement of 126% over DQN after normalizing with respect to a random agent. Additional improvements can be gained by including n-step updates.\n",
    "\n",
    "For a deeper dive into the C51 algorithm, see [A Distributional Perspective on Reinforcement Learning (2017)](https://arxiv.org/pdf/1707.06887.pdf)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DQN C51/Rainbow Tutorial.ipynb",
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "gym_simulator",
   "language": "python",
   "name": "gym_simulator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
