{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment installation\n",
    "Install all environment dependencies using `pip install -e .`\n",
    "\n",
    "*Reference the README for further details*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment states\n",
    "\n",
    "The environment monitors the following variables:\n",
    "* (HA)2(org)\n",
    "* H+ Extraction\n",
    "* H+ Scrub\n",
    "* H+ Strip\n",
    "* OA Extraction\n",
    "* OA Scrub\n",
    "* OA Strip \n",
    "* Recycle\n",
    "* Extraction\n",
    "* Scrub \n",
    "* Strip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the module\n",
    "1. Make the necessary imports\n",
    "2. Create environment instance using gym.make()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#manage imports\n",
    "import gym\n",
    "import gym_solventx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_solventx-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opitons that can be passed while creating environment\n",
    "1. Goals list (include 'Purity, 'Recovery, 'Stages', 'OA Extraction', 'OA Scrub', 'OA Strip', 'Recycle', 'Profit')\n",
    "2. Discrete reward\n",
    "3. Bounds_file (a file that can restrict environment upper and lower limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_solventx-v0', \n",
    "      goals_list=['Purity', 'Recovery'], \n",
    "      bounds_file='gym_solventx/envs/methods/input/bounds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('gym_solventx-v0',\n",
    "      DISCRETE_REWARD=True,\n",
    "      goals_list=['Purity', 'Recovery'], \n",
    "      bounds_file='gym_solventx/envs/methods/input/bounds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#additional goals list options include 'Purity, 'Recovery, 'Stages', 'OA Extraction', 'OA Scrub', 'OA Strip', 'Recycle', 'Profit'\n",
    "env = gym.make('gym_solventx-v0', \n",
    "      goals_list=['Purity', 'Recovery', 'Recycle'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset environment and perform actions\n",
    "\n",
    "1. There are 23 discrete actions.  \n",
    "    0 - Increase (HA)2(org)  \n",
    "    1 - Decrease (HA)2(org)  \n",
    "    2 - Increase H+ Extraction  \n",
    "    3 - Decrease H+ Extraction  \n",
    "    4 - Increase H+ Scrub  \n",
    "    5 - Decrease H+ Scrub  \n",
    "    6 - Increase H+ Strip  \n",
    "    7 - Decrease H+ Strip  \n",
    "    8 - Increase OA Extraction  \n",
    "    9 - Decrease OA Extraction  \n",
    "    10 - Increase OA Scrub  \n",
    "    11 - Decrease OA Scrub  \n",
    "    12 - Increase OA Strip  \n",
    "    13 - Decrease OA Strip  \n",
    "    14 - Increase Recycle  \n",
    "    15 - Decrease Recycle  \n",
    "    16 - Increase Extraction Stages  \n",
    "    17 - Decrease Extraction Stages  \n",
    "    18 - Increase Scrub Stages  \n",
    "    19 - Decrease Scrub Stages  \n",
    "    20 - Increase Strip Stages  \n",
    "    21 - Decrease Strip Stages  \n",
    "    22 - Do Nothing\n",
    "2. Before we can apply an action we need to call the reset() method.\n",
    "3. An action can be applied by calling the step() method.\n",
    "4. The result of the action can be observed using the render method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [22,0,1] #do nothing, increase (HA)2(org), decrease (HA)2(org)\n",
    "envstate =  env.reset()\n",
    "env.render() #observe initial configuration\n",
    "\n",
    "for action in actions:\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invalid actions\n",
    "Some actions can cause the environment to go to invalid states. In such cases the step() method executes but no change is applied to the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [17,17,17,17,17,17] #decrease extration stages\n",
    "envstate =  env.reset()\n",
    "env.render() #observe initial configuration\n",
    "\n",
    "for action in actions:\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from environment action space\n",
    "1. We can randomly sample from the discrete action space.\n",
    "2. The below code shows 15 random actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "for _ in range(1):  #epoch number\n",
    "  done = False\n",
    "  envstate = env.reset()\n",
    "  for index in range(15): #action count\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    all_rewards.append(reward)\n",
    "    \n",
    "  print('All Rewards:', all_rewards)\n",
    "  all_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test environment to completion\n",
    "---\n",
    "\n",
    "The maximum action count for the environment is 500 actions.\n",
    "\n",
    "***Note: this takes a significant amount of time***\n",
    "\n",
    "However if you would like to run an environment to completion, here is example code (*note: the outputs folder is created at the end of an episode*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "for _ in range(1):  #epoch number\n",
    "  done = False\n",
    "  action_count = 0\n",
    "  envstate = env.reset()\n",
    "  env.render() #observe initial configration\n",
    "  while not done: #take action\n",
    "    action_count += 1\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "    \n",
    "    if action_count % 25 == 0: #render every 25 steps\n",
    "        env.render()\n",
    "    all_rewards.append(reward)\n",
    "    \n",
    "  print('All Rewards:', all_rewards)\n",
    "  all_rewards = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Action Stats\n",
    "===\n",
    "\n",
    "1. To get the stats of actions taken during an episode you can call the `get_stats()` function. This returns a dictionary with the stats.\n",
    "\n",
    "2. To get a visual representation, instead pass the `SHOW_PLOT=True` parameter\n",
    "\n",
    "3. You can also get the best reward the environment reached during an episode by calling `env.best_reward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = env.get_stats()\n",
    "print('Stats:', stats)\n",
    "print('Actions:', sum(stats.values()), end='\\n\\n')\n",
    "\n",
    "stats = env.get_stats(SHOW_PLOT=True)\n",
    "\n",
    "print('Best reward:', env.best_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render options\n",
    "===  \n",
    "To utilize graph generation during testing you can add the option `create_graph_every=n` to the render method which will create a graph and save it every n steps/actions\n",
    "\n",
    "Additionally, to force the environment to render quietly you can change the mode to 'file' (default 'human') like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs  = []\n",
    "envstate = env.reset()\n",
    "\n",
    "actions = [0, 1]\n",
    "for action in actions:\n",
    "    env.step(action)\n",
    "    output = env.render(mode='file', create_graph_every=1) #graph generation in ./output/graphs\n",
    "    \n",
    "    #store output for later demonstration\n",
    "    outputs.append(output)\n",
    "    \n",
    "print('Done.')\n",
    "\n",
    "#print outputs\n",
    "for log in outputs:\n",
    "    print(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph Options\n",
    "===\n",
    "1. To render a single graph you can call the `create_graph()` function independently. This will model the current configuration.\n",
    "\n",
    "2. Additional options for the `create_graph()` function include `render` which will immediately render the graph in your default pdf viewer (***note this file must be closed to render a new graph, the pdf however will be saved regardless***)\n",
    "And `filename=desired_name_of_file` if you would like the graph to have a specific name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envstate = env.reset()\n",
    "env.create_graph() #graph generated in './output/graphs/ssgraph.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envstate = env.reset()\n",
    "env.create_graph(render=True, filename='demo_graph') #graph generated in './output/graphs/demo_graph.pdf'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
