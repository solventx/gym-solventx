{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from absl import logging\n",
    "import gym\n",
    "import gym_solventx \n",
    "\n",
    "import gin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tf_agents.agents.ppo import ppo_agent\n",
    "from tf_agents.drivers import dynamic_episode_driver\n",
    "from tf_agents.environments import parallel_py_environment\n",
    "#from tf_agents.environments import suite_mujoco\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.eval import metric_utils\n",
    "from tf_agents.metrics import tf_metrics\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import actor_distribution_rnn_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.networks import value_rnn_network\n",
    "from tf_agents.policies import policy_saver\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.utils import common\n",
    "\n",
    "logging.set_verbosity(logging.INFO)\n",
    "tf.compat.v1.enable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_dir\n",
    "env_name='gym_solventx-v0'\n",
    "env_load_fn=suite_gym.load\n",
    "random_seed=0\n",
    "actor_fc_layers=(200, 100)\n",
    "value_fc_layers=(200, 100)\n",
    "use_rnns=False\n",
    "# Params for collect\n",
    "num_environment_steps=10000000\n",
    "collect_episodes_per_iteration=1\n",
    "num_parallel_environments=1\n",
    "replay_buffer_capacity=1001  # Per-environment\n",
    "# Params for train\n",
    "num_epochs=25\n",
    "learning_rate=1e-4\n",
    "# Params for eval\n",
    "num_eval_episodes=30\n",
    "eval_interval=500\n",
    "# Params for summaries and logging\n",
    "train_checkpoint_interval=500\n",
    "policy_checkpoint_interval=500\n",
    "log_interval=25\n",
    "summary_interval=50\n",
    "summaries_flush_secs=1\n",
    "use_tf_functions=True\n",
    "debug_summaries=False\n",
    "summarize_grads_and_vars=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.set_random_seed(random_seed)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
    "#tf_env = tf_py_environment.TFPyEnvironment(parallel_py_environment.ParallelPyEnvironment(\n",
    "                                           #[lambda: env_load_fn(env_name)] * num_parallel_environments))\n",
    "#train_py_env = suite_gym.load(env_name)\n",
    "tf_env = tf_py_environment.TFPyEnvironment(env_load_fn(env_name))\n",
    "\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir =  'train'\n",
    "eval_dir = 'eval'\n",
    "train_summary_writer = tf.compat.v2.summary.create_file_writer(train_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "train_summary_writer.set_as_default()\n",
    "\n",
    "eval_summary_writer = tf.compat.v2.summary.create_file_writer(eval_dir, flush_millis=summaries_flush_secs * 1000)\n",
    "eval_metrics = [\n",
    "      tf_metrics.AverageReturnMetric(buffer_size=num_eval_episodes),\n",
    "      tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = tf.compat.v1.train.get_or_create_global_step()\n",
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "          tf_env.observation_spec(),\n",
    "          tf_env.action_spec(),\n",
    "          fc_layer_params=actor_fc_layers)\n",
    "value_net = value_network.ValueNetwork(tf_env.observation_spec(), fc_layer_params=value_fc_layers)\n",
    "\n",
    "tf_agent = ppo_agent.PPOAgent(\n",
    "        tf_env.time_step_spec(),\n",
    "        tf_env.action_spec(),\n",
    "        optimizer,\n",
    "        actor_net=actor_net,\n",
    "        value_net=value_net,\n",
    "        num_epochs=num_epochs,\n",
    "        debug_summaries=debug_summaries,\n",
    "        summarize_grads_and_vars=summarize_grads_and_vars,\n",
    "        train_step_counter=global_step)\n",
    "tf_agent.initialize()\n",
    "environment_steps_metric = tf_metrics.EnvironmentSteps()\n",
    "step_metrics = [tf_metrics.NumberOfEpisodes(),environment_steps_metric,]\n",
    "train_metrics = step_metrics + [\n",
    "        tf_metrics.AverageReturnMetric(\n",
    "            batch_size=num_parallel_environments),\n",
    "        tf_metrics.AverageEpisodeLengthMetric(\n",
    "            batch_size=num_parallel_environments),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_policy = tf_agent.policy\n",
    "collect_policy = tf_agent.collect_policy\n",
    "\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(tf_agent.collect_data_spec,\n",
    "        batch_size=num_parallel_environments,\n",
    "        max_length=replay_buffer_capacity)\n",
    "collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "        tf_env,\n",
    "        collect_policy,\n",
    "        observers=[replay_buffer.add_batch] + train_metrics,\n",
    "        num_episodes=collect_episodes_per_iteration)\n",
    "def train_step():\n",
    "    trajectories = replay_buffer.gather_all()\n",
    "    return tf_agent.train(experience=trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_time = 0\n",
    "train_time = 0\n",
    "timed_at_step = global_step.numpy()\n",
    "\n",
    "while environment_steps_metric.result() < num_environment_steps:\n",
    "    global_step_val = global_step.numpy()\n",
    "    \"\"\"\n",
    "    if global_step_val % eval_interval == 0:\n",
    "        metric_utils.eager_compute(eval_metrics,\n",
    "                                   eval_tf_env,\n",
    "                                   eval_policy,\n",
    "                                   num_episodes=num_eval_episodes,\n",
    "                                   train_step=global_step,\n",
    "                                   summary_writer=eval_summary_writer,\n",
    "                                   summary_prefix='Metrics', )\n",
    "    \"\"\"\n",
    "    print(f'Starting collect for step {global_step_val}.')\n",
    "    start_time = time.time()\n",
    "    collect_driver.run()\n",
    "    collect_time += time.time() - start_time\n",
    "    print(f'Starting train for step {global_step_val}.')\n",
    "    start_time = time.time()\n",
    "    total_loss, _ = train_step()\n",
    "    replay_buffer.clear()\n",
    "    train_time += time.time() - start_time\n",
    "\n",
    "    #for train_metric in train_metrics:\n",
    "    #    train_metric.tf_summaries(train_step=global_step, step_metrics=step_metrics)\n",
    "\n",
    "    if global_step_val % log_interval == 0:\n",
    "        logging.info('step = %d, loss = %f', global_step_val, total_loss)\n",
    "        steps_per_sec = ((global_step_val - timed_at_step) / (collect_time + train_time))\n",
    "        logging.info('%.3f steps/sec', steps_per_sec)\n",
    "        logging.info('collect_time = {}, train_time = {}'.format(collect_time, train_time))\n",
    "        \"\"\"\n",
    "        with tf.compat.v2.summary.record_if(True):\n",
    "            tf.compat.v2.summary.scalar(name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n",
    "\n",
    "        #if global_step_val % train_checkpoint_interval == 0:\n",
    "            train_checkpointer.save(global_step=global_step_val)\n",
    "\n",
    "        if global_step_val % policy_checkpoint_interval == 0:\n",
    "            policy_checkpointer.save(global_step=global_step_val)\n",
    "            saved_model_path = os.path.join(saved_model_dir, 'policy_' + ('%d' % global_step_val).zfill(9))\n",
    "            saved_model.save(saved_model_path)\n",
    "        \"\"\"\n",
    "        timed_at_step = global_step_val\n",
    "        collect_time = 0\n",
    "        train_time = 0\n",
    "\"\"\"\n",
    "# One final eval before exiting.\n",
    "metric_utils.eager_compute(\n",
    "        eval_metrics,\n",
    "        eval_tf_env,\n",
    "        eval_policy,\n",
    "        num_episodes=num_eval_episodes,\n",
    "        train_step=global_step,\n",
    "        summary_writer=eval_summary_writer,\n",
    "        summary_prefix='Metrics',\n",
    "    )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gym_simulator",
   "language": "python",
   "name": "gym_simulator"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
